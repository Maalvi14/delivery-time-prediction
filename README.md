# Delivery Time Prediction

A comprehensive machine learning project for predicting food delivery times in urban settings. This project addresses the critical business problem of late deliveries that hurt customer trust, increase support costs, and risk customer churn.


## ğŸš€ Quick Start

### Prerequisites

**macOS Users - Important:** You need to install `libomp` for LightGBM/XGBoost to work properly:
```bash
brew install libomp
```

### Installation

This project uses `uv` for dependency management. We strongly recommend using `uv run` instead of `python` or `pip` commands.

1. **Clone the repository:**
```bash
git clone <repository-url>
cd delivery-time-prediction
```

2. **Install dependencies with uv:**
```bash
# Install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh

# or install via Homebrew with
brew install uv

# Install project dependencies
uv sync
```

### Training a Model

**Recommended approach:** Use the provided training script with `uv run`:

```bash
# Train the model using the example script
uv run model_pipeline/examples/train_model.py
```

This will:
- Load the data from `data/Food_Delivery_Times.csv`
- Run the complete ML pipeline
- Train and compare 12 different models
- Save the best model to `models/`
- Generate performance reports




## ğŸŒ API Usage

### Start the API Server

```bash
# Start the FastAPI server
uv run run_api.py
```

The server will start on `http://localhost:8000` by default. API documentation is available at `http://localhost:8000/docs`.

### Model Auto-Discovery

The API automatically discovers and uses the first available model in the `models/` directory:
- **Auto-discovery**: Scans `models/` directory for `.pkl` files
- **Consistent ordering**: Uses alphabetical sorting for predictable model selection
- **Fallback**: Falls back to `ridge_regression.pkl` if no models found
- **Override**: Can be overridden with `MODEL_PATH` environment variable

### API Endpoints

#### Health Check
```bash
GET /health
```

#### Model Information
```bash
GET /model/info
```

#### Single Prediction
```bash
POST /predict
```

**Request Body:**
```json
{
  "Distance_km": 10.5,
  "Weather": "Clear",
  "Traffic_Level": "Medium",
  "Time_of_Day": "Evening",
  "Vehicle_Type": "Bike",
  "Preparation_Time_min": 15.0,
  "Courier_Experience_yrs": 3.5
}
```

#### Batch Predictions
```bash
POST /predict/batch
```

### Example Usage with curl

```bash
# Health check
curl http://localhost:8000/health

# Single prediction
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "Distance_km": 10.5,
    "Weather": "Clear",
    "Traffic_Level": "Medium",
    "Time_of_Day": "Evening",
    "Vehicle_Type": "Bike",
    "Preparation_Time_min": 15.0,
    "Courier_Experience_yrs": 3.5
  }' | jq
```

## Available Reports & Documentation

This project includes comprehensive analysis and documentation:

### ğŸ“ˆ Analysis Reports
- **[EDA Report](EDA_report.md)** - Complete exploratory data analysis with business insights
- **[Model Notes](model_notes.md)** - Detailed model development process and findings
- **[Pipeline Summary](PIPELINE_SUMMARY.md)** - Technical implementation overview
- **[Error Insights](error_insights.md)** - Analysis of prediction errors and patterns
- **[Explainability](explainability.md)** - Model interpretability and feature importance
- **[Strategic Reflections](strategic_reflections.md)** - Strategic insights and business recommendations

### ğŸ—„ï¸ SQL Analysis
- **[SQL Queries](sql/sql_queries.sql)** - Comprehensive SQL queries for data analysis
- **[SQL Insights](sql/sql_insights.md)** - Key findings from SQL analysis

### ğŸ““ Jupyter Notebooks
- **[EDA.ipynb](notebooks/EDA.ipynb)** - Interactive exploratory data analysis

### ğŸ“ Generated Images
All analysis plots are saved in `notebooks/images/` including:
- Feature distribution analysis
- Correlation heatmaps
- Model performance comparisons
- Error distribution analysis
- Feature importance plots



## Project Structure

```
delivery-time-prediction/
â”œâ”€â”€ data/                                 # Data files
â”‚   â”œâ”€â”€ Food_Delivery_Times.csv           # Raw dataset
â”‚   â””â”€â”€ model_comparison_results.csv      # Comparison Results of all 12 models
â”œâ”€â”€ notebooks/                            # Jupyter notebooks
â”‚   â”œâ”€â”€ EDA.ipynb                         # Exploratory Data Analysis
â”‚   â””â”€â”€ images/                           # Generated analysis plots
â”œâ”€â”€ model_pipeline/                       # Production ML pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                         # Configuration parameters
â”‚   â”œâ”€â”€ preprocessing.py                  # Data preprocessing
â”‚   â”œâ”€â”€ feature_engineering.py            # Feature engineering
â”‚   â”œâ”€â”€ models.py                         # Model training & evaluation
â”‚   â”œâ”€â”€ predict.py                        # Prediction interface
â”‚   â”œâ”€â”€ pipeline.py                       # Main pipeline orchestrator
â”‚   â”œâ”€â”€ utils.py                          # Utility functions
â”‚   â”œâ”€â”€ README.md                         # Pipeline documentation
â”‚   â””â”€â”€ examples/                         # Usage examples
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train_model.py                # Training script
â”‚       â”œâ”€â”€ make_predictions.py           # Prediction examples
â”‚       â””â”€â”€ custom_pipeline.py            # Custom pipeline examples
â”œâ”€â”€ api/                                  # FastAPI application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                            # Main FastAPI application
â”‚   â”œâ”€â”€ models.py                         # Pydantic models
â”‚   â”œâ”€â”€ predictor_service.py              # Predictor service wrapper
â”‚   â””â”€â”€ config.py                         # API configuration
â”œâ”€â”€ models/                               # Saved models
â”‚   â””â”€â”€ ridge_regression.pkl              # Best performing model
â”œâ”€â”€ results/                              # Model results
â”‚   â”œâ”€â”€ model_comparison.csv              # Model comparison results
â”‚   â””â”€â”€ overfitting_analysis.csv          # Overfitting analysis results
â”œâ”€â”€ sql/                                  # SQL analysis
â”‚   â”œâ”€â”€ sql_queries.sql                   # SQL queries for data analysis
â”‚   â””â”€â”€ sql_insights.md                   # SQL analysis insights
â”œâ”€â”€ tests/                                # Test files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_pipeline.py                  # Pipeline tests
â”‚   â””â”€â”€ test_prediction.py                # Prediction tests
â”œâ”€â”€ run_api.py                            # API server startup script
â”œâ”€â”€ main.py                               # Main application entry point
â”œâ”€â”€ pyproject.toml                        # Project dependencies
â”œâ”€â”€ uv.lock                               # Dependency lock file
â”œâ”€â”€ LICENSE                               # License file
â”œâ”€â”€ EDA_report.md                         # EDA analysis report
â”œâ”€â”€ error_insights.md                     # Error analysis insights
â”œâ”€â”€ explainability.md                     # Model explainability report
â”œâ”€â”€ model_notes.md                        # Model development notes
â”œâ”€â”€ PIPELINE_SUMMARY.md                   # Pipeline summary
â”œâ”€â”€ strategic_reflections.md              # Strategic insights
â”œâ”€â”€ v1 DS Technical Assessment.pdf        # Technical assessment document
â””â”€â”€ README.md                             # This file
```

## ğŸ§ª Testing

Run the test script to verify everything works:

```bash
uv run python test_pipeline.py
```

This will:
1. Initialize the pipeline
2. Load and process data
3. Train models
4. Make test predictions
5. Verify all components work correctly

## ğŸ“¦ Dependencies

Core dependencies managed via `pyproject.toml`:
- pandas >= 2.3.3
- numpy (via pandas)
- scikit-learn >= 1.7.2
- xgboost >= 3.0.5
- lightgbm >= 4.6.0
- matplotlib >= 3.10.7
- seaborn >= 0.13.2
- scipy >= 1.16.2
- fastapi >= 0.119.0
- uvicorn >= 0.38.0

**Note:** This project requires Python >= 3.12


## ğŸ“ˆ Model Performance Comparison

| Rank | Model | Test RÂ² | Test RMSE | Test MAE | MAPE (%) |
|------|-------|---------|-----------|----------|----------|
| 1 | Ridge Regression | 0.8199 | 8.98 | 6.04 | 10.77 |
| 2 | Linear Regression | 0.8193 | 9.00 | 6.06 | 10.83 |
| 3 | Lasso Regression | 0.8032 | 9.39 | 6.55 | 12.76 |
| 4 | LightGBM | 0.7900 | 9.70 | 6.90 | 12.47 |
| 5 | Random Forest | 0.7855 | 9.80 | 7.08 | 13.35 |

*Full and more detailed results available in `data/model_comparison_results.csv`* after running training pipeline.

## ğŸ”® Future Improvements

- [ ] Hyperparameter tuning with GridSearch/RandomSearch
- [ ] Feature selection optimization
- [ ] Ensemble methods
- [x] Real-time prediction API
- [ ] Model monitoring and drift detection
- [ ] A/B testing framework
- [ ] Integration with delivery platforms
- [ ] Model versioning and rollback capabilities

## ğŸ¤ Contributing

Contributions are welcome. Please feel free to submit a Pull Request.

## ğŸ“„ License

See LICENSE file for details.

## ğŸ‘¥ Authors

Data Science Team

## ğŸ“ Support

For issues or questions:
1. Check the documentation in `model_pipeline/README.md`
2. Review examples in `model_pipeline/examples/`
3. Run `uv run python test_pipeline.py` to diagnose issues
4. Open an issue on GitHub
